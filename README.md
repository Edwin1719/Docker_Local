# Agente Local Docker Model Runner con Capacidad de B√∫squeda Web

Este proyecto implementa un entorno para interactuar con Modelos de Lenguaje Grandes (LLMs) de forma local, potenciado por un agente ligero de Business Intelligence (BI) con capacidad para realizar b√∫squedas en la web en tiempo real. La ejecuci√≥n del LLM se realiza a trav√©s de la funcionalidad "Docker Model Runner" de Docker Desktop, que proporciona un servidor de inferencia de modelos en un puerto TCP local.

## üìú Descripci√≥n

El sistema permite a los usuarios enviar consultas a un modelo de lenguaje que se ejecuta localmente. Se incluyen dos scripts principales:

1.  `main.py`: Un cliente simple para enviar prompts directos al modelo y recibir respuestas.
2.  `agent_search.py`: Un agente m√°s sofisticado que puede interpretar una consulta, decidir si necesita informaci√≥n externa, realizar una b√∫squeda web utilizando la API de Tavily y, finalmente, sintetizar una respuesta informada basada en los datos recopilados.

Adicionalmente, el entorno se complementa con una interfaz de usuario web (`Open-WebUI`) para la interacci√≥n conversacional directa con el modelo.

## üå± Evoluci√≥n del Proyecto

Este proyecto se desarroll√≥ en tres fases clave, demostrando un camino progresivo desde una simple conexi√≥n hasta una aplicaci√≥n web funcional.

### Fase 1: Conexi√≥n Directa (`main.py`)
El punto de partida fue establecer una comunicaci√≥n b√°sica con el modelo de lenguaje local servido a trav√©s del "Docker Model Runner". El script `main.py` es un testimonio de esta fase: un c√≥digo sencillo y directo que env√≠a un prompt al modelo y recibe una respuesta. Sirve como la prueba de concepto fundamental de la conexi√≥n.

### Fase 2: Un Agente con Herramientas (`agent_search.py`)
Una vez establecida la conexi√≥n, el siguiente paso fue dotar al sistema de mayor inteligencia. Se cre√≥ `agent_search.py`, un agente simple pero funcional. Este agente introdujo la capacidad de "razonamiento": pod√≠a analizar una pregunta y decidir aut√≥nomamente si responder con su conocimiento base o utilizar una herramienta externa (la API de Tavily) para realizar una b√∫squeda en la web y obtener informaci√≥n actualizada.

### Fase 3: Interfaz Gr√°fica con Streamlit (`app.py`)
La fase final consisti√≥ en hacer que el agente fuera accesible y f√°cil de usar para cualquier persona. Se desarroll√≥ `app.py`, una aplicaci√≥n web completa construida con Streamlit. Esta interfaz no solo proporciona un chat interactivo, sino que tambi√©n implementa caracter√≠sticas profesionales como:
- **Historial de conversaci√≥n** para mantener el contexto (aunque con limitaciones intencionadas para mejorar el rendimiento).
- **Respuestas en streaming** para una experiencia de usuario fluida y en tiempo real.
- **Un dise√±o profesional** con una barra lateral informativa y una disposici√≥n clara.
- **Manejo de errores y estados de carga** para una mayor robustez.

## üèóÔ∏è Arquitectura del Sistema

La arquitectura de este proyecto se compone de tres partes principales que operan de manera coordinada:

![Arquitectura del Sistema](https://i.imgur.com/your-architecture-diagram.png)  <!-- Reemplazar con un diagrama real si es posible -->

1.  **Motor del LLM (Docker Model Runner)**
    *   **Tecnolog√≠a**: El servicio que ejecuta el modelo de lenguaje es proporcionado por la funcionalidad **"Docker Model Runner"** de Docker Desktop. Esta caracter√≠stica permite a Docker Desktop ejecutar motores de inferencia acelerados por GPU y exponerlos en el host.
    *   **Configuraci√≥n**: El "Docker Model Runner" est√° habilitado para el soporte TCP en el lado del host, exponiendo el servicio del LLM en el puerto **`12434`** (`http://localhost:12434`).
    *   **Modelo**: El modelo `ai/gemma3:4B` es descargado y gestionado directamente por este "Docker Model Runner".

2.  **Scripts de Python (Clientes de la API)**
    *   Los scripts `main.py` y `agent_search.py` act√∫an como clientes que consumen la API del LLM.
    *   Env√≠an peticiones HTTP POST al endpoint `http://localhost:12434/v1/chat/completions` para interactuar con el modelo.
    *   `agent_search.py` adem√°s se conecta a la API externa de [Tavily AI](https://tavily.com/) para sus capacidades de b√∫squeda web.

3.  **Interfaz de Usuario (Open-WebUI)**
    *   **Tecnolog√≠a**: Es una aplicaci√≥n web que se ejecuta en un **contenedor Docker** a partir de la imagen `ghcr.io/open-webui/open-webui:main`.
    *   **Funci√≥n**: Provee una interfaz gr√°fica de chat (accesible en `http://localhost:3000`) para una interacci√≥n m√°s amigable con el modelo.
    *   **Conexi√≥n**: Esta interfaz est√° configurada para comunicarse internamente con el "Docker Model Runner" (posiblemente a trav√©s de `model-runner.docker.internal:80`), lo que permite que Open-WebUI funcione como un frontend para el LLM.

## üîß Configuraci√≥n del Entorno de Docker

Antes de la instalaci√≥n del proyecto, es crucial configurar correctamente Docker Desktop para que pueda servir el modelo de lenguaje.

### 1. Configuraci√≥n del Docker Model Runner

El "Docker Model Runner" es una caracter√≠stica de Docker Desktop que gestiona la ejecuci√≥n de modelos de IA. Sigue estos pasos para configurarlo:

1.  Abre **Docker Desktop** y ve a **Settings** (el icono del engranaje).
2.  Navega a la secci√≥n **AI/ML**.
3.  Activa la opci√≥n principal: **"Enable Docker Model Runner"**.
4.  **Activa la aceleraci√≥n por GPU (MUY RECOMENDADO)**: Marca la casilla **"Enable GPU-backed inference"**. Esto es fundamental para obtener un buen rendimiento y evitar timeouts.
    *   *Nota: Requiere una tarjeta gr√°fica NVIDIA compatible con los drivers de CUDA correctamente instalados en tu sistema.*
5.  **Habilita la conexi√≥n local**: Marca la casilla **"Enable host-side TCP support"**. Esto expone el modelo en un puerto de tu m√°quina para que nuestras aplicaciones puedan conectarse.
6.  **Configura el Puerto**: En el campo **Port**, aseg√∫rate de que el valor sea **`12434`**. Este es el puerto que nuestro proyecto espera.
7.  **Or√≠genes CORS**: Deja el valor por defecto (`All` o `*`) para desarrollo local.

Tu configuraci√≥n deber√≠a verse similar a esto:

```text
Docker Model Runner
[x] Enable Docker Model Runner

    Enable GPU-accelerated inference engines...

[x] Enable host-side TCP support
    Port: 12434

    CORS allowed origins: All

[x] Enable GPU-backed inference
```

### 2. Descarga del Modelo de Lenguaje

Con el "Docker Model Runner" activado, puedes descargar modelos directamente desde Docker Hub a trav√©s de la interfaz de Docker Desktop.

1.  Abre **Docker Desktop**.
2.  En el panel de la izquierda, busca y haz clic en la secci√≥n **Models**.
3.  Dentro de la secci√≥n de "Models", ver√°s una lista de modelos disponibles de Docker Hub. Usa la barra de b√∫squeda para encontrar el modelo que necesitamos para este proyecto:
    ```
    ai/gemma3:4B
    ```
4.  Una vez encontrado, haz clic en el bot√≥n **Download** (o similar) junto al nombre del modelo.
5.  Espera a que Docker Desktop complete la descarga. El progreso se mostrar√° en la misma interfaz.

Este proceso descargar√° el modelo y lo har√° disponible para que el "Docker Model Runner" lo utilice, asegurando que est√© listo cuando nuestra aplicaci√≥n lo solicite.

## ‚öôÔ∏è Prerrequisitos

Antes de comenzar, aseg√∫rate de tener instalado lo siguiente:

*   **Python 3.8+**
*   **Docker Desktop**: Necesario. Aseg√∫rate de tener la funcionalidad "Docker Model Runner" habilitada y configurada para exponer el puerto `12434` en el host. Esta configuraci√≥n suele encontrarse en las preferencias de Docker Desktop, en la secci√≥n "AI/ML" o similar.
*   **Modelo de Lenguaje**: El modelo `ai/gemma3:4B` ser√° descargado y gestionado autom√°ticamente por el "Docker Model Runner" cuando se le solicite, o puedes iniciarlo directamente desde la terminal si el "Docker Model Runner" lo permite.
*   **Docker Desktop**: Necesario si deseas utilizar la interfaz gr√°fica de Open-WebUI. Desc√°rgalo desde [Docker Hub](https://www.docker.com/products/docker-desktop/).

## üöÄ Instalaci√≥n

Sigue estos pasos para configurar el proyecto en tu m√°quina local:

1.  **Aseg√∫rate de que Docker Desktop est√© instalado y el "Docker Model Runner" est√© habilitado**:
    *   Verifica que Docker Desktop est√© ejecut√°ndose en tu sistema.
    *   En las preferencias de Docker Desktop, navega a la secci√≥n de "AI/ML" o similar y aseg√∫rate de que el "Docker Model Runner" est√© habilitado.
    *   Confirma que el puerto `12434` est√© configurado y expuesto para el "host-side TCP support".

2.  **Clona el repositorio** (o descarga los archivos en un directorio local):
    ```bash
    git clone <URL-del-repositorio>
    cd <nombre-del-directorio>
    ```

3.  **Crea un entorno virtual** (recomendado):
    ```bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    ```

4.  **Instala las dependencias** de Python:
    ```bash
    pip install -r requirements.txt
    ```
5.  **Configura las variables de entorno**:
    *   Crea un archivo llamado `.env` en la ra√≠z del proyecto.
    *   A√±ade tu clave de API de Tavily. El archivo debe lucir as√≠:
    ```ini
    TAVILY_API_KEY="tu-clave-de-api-de-tavily"
    MODEL_URL="http://localhost:12434/v1/chat/completions"
    MODEL_NAME="ai/gemma3:4B"
    ```

## ‚ñ∂Ô∏è Uso

Aseg√∫rate de que el **"Docker Model Runner" est√© activo y gestionando el modelo `ai/gemma3:4B`** en el puerto `12434`.

### Ejecutar el script simple

Este script enviar√° un prompt predefinido al modelo y mostrar√° la respuesta.

```bash
python main.py
```

### Ejecutar el Agente de B√∫squeda

Este script te pedir√° que introduzcas una pregunta. El agente procesar√° la pregunta, buscar√° en la web si es necesario y generar√° un reporte.

```bash,
python agent_search.py
```
> **Pregunta:** `[Escribe tu pregunta aqu√≠ y presiona Enter]`


### Acceder a la Interfaz Web (Opcional)

Si tienes Docker y has iniciado el contenedor de `open-webui`, puedes acceder a la interfaz de chat abriendo tu navegador y visitando:
[**http://localhost:3000**](http://localhost:3000)

---
*Este README fue generado para documentar la arquitectura y el funcionamiento del proyecto de agente local.*

## üí° Casos de Uso Potenciales

Esta arquitectura de LLM local abre un amplio abanico de posibilidades para el desarrollo de aplicaciones inteligentes y seguras:

*   **Desarrollo R√°pido de Agentes de IA**: Prototipar y depurar agentes complejos (como `agent_search.py`) que requieren m√∫ltiples interacciones con el LLM, aprovechando la baja latencia de la ejecuci√≥n local.
*   **Herramientas Internas y Asistentes de C√≥digo**: Crear herramientas para equipos de desarrollo que ayuden a generar c√≥digo, refactorizar, explicar fragmentos complejos o buscar en la documentaci√≥n interna, todo sin que el c√≥digo fuente salga de la red local.
*   **An√°lisis y Resumen de Documentos Confidenciales**: Construir aplicaciones que procesen documentos sensibles (informes financieros, legales, m√©dicos) de forma segura, garantizando que ninguna informaci√≥n se env√≠e a servicios en la nube.
*   **Aplicaciones "Offline-First"**: Desarrollar aplicaciones que puedan funcionar sin conexi√≥n a internet, ya que el motor de inferencia principal reside en la m√°quina local.
*   **Educaci√≥n e Investigaci√≥n**: Aprender sobre el funcionamiento de los LLMs, la ingenier√≠a de prompts y la arquitectura de agentes en un entorno controlado y sin coste por uso.

## ‚ú® Capacidades y Beneficios de esta Arquitectura



*   **Privacidad y Seguridad de Datos**: Al ejecutarse el modelo de forma local, toda la informaci√≥n procesada, incluyendo prompts y respuestas, permanece en tu m√°quina. Esto es fundamental para manejar datos sensibles, c√≥digo propietario o informaci√≥n personal.

*   **Reducci√≥n de Costes**: Elimina la dependencia de APIs de pago por uso. El coste se limita a la inversi√≥n inicial en hardware y el consumo el√©ctrico, permitiendo un uso intensivo y experimentaci√≥n sin preocuparse por la factura del servicio.

*   **Baja Latencia y Alto Rendimiento**: La ejecuci√≥n local, especialmente con la aceleraci√≥n por GPU que facilita el "Docker Model Runner", ofrece tiempos de respuesta casi instant√°neos en comparaci√≥n con las llamadas a APIs remotas, lo que es crucial para aplicaciones interactivas.

*   **Control y Personalizaci√≥n Total**: Tienes control absoluto sobre la versi√≥n del modelo, la configuraci√≥n del servidor y el entorno de ejecuci√≥n. Permite una personalizaci√≥n que los proveedores de API no ofrecen.

*   **Funcionamiento sin Conexi√≥n**: El n√∫cleo de la funcionalidad del LLM no requiere una conexi√≥n a internet, lo que habilita el desarrollo de aplicaciones robustas que pueden operar en entornos con conectividad limitada o nula (la b√∫squeda web del agente s√≠ requiere internet).

*   **Despliegue Simplificado con Docker**: El "Docker Model Runner" abstrae la complejidad de configurar y ejecutar motores de inferencia, permitiendo a los desarrolladores centrarse en la aplicaci√≥n en lugar de en la infraestructura del modelo.



## üë§ Contacto del Desarrollador



Este proyecto fue desarrollado por:



*   **Nombre:** Edwin Quintero Alzate

*   **GitHub:** [Edwin1719](https://github.com/Edwin1719)

*   **LinkedIn:** [Edwin Quintero Alzate](https://www.linkedin.com/in/edwinquintero0329/)

*   **Email:** databiq29@gmail.com



---

*Este README fue generado para documentar la arquitectura y el funcionamiento del proyecto de agente local.*


